{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aiayn - Attention is all you Need\n",
    "![alt text](images/aiayn/aiayn.png \"Architecture of AIAYN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
    "learned and fixed [8].\n",
    "In this work, we use sine and cosine functions of different frequencies:\n",
    "P E(pos,2i) = sin(pos/100002i/dmodel)\n",
    "P E(pos,2i+1) = cos(pos/100002i/dmodel)\n",
    "\n",
    "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
    "corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
    "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\n",
    "P Epos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "def positional_encoding(length: int, depth: int) -> torch.Tensor:\n",
    "    depth = depth // 2\n",
    "\n",
    "    positions = torch.arange(length, dtype=torch.float32).unsqueeze(1)  # (seq, 1)\n",
    "    depths = torch.arange(depth, dtype=torch.float32).unsqueeze(0) / depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)  # (1, depth)\n",
    "    angle_rads = positions * angle_rates  # (pos, depth)\n",
    "\n",
    "    pos_encoding = torch.cat((torch.sin(angle_rads), torch.cos(angle_rads)), dim=-1)  # (pos, depth*2)\n",
    "\n",
    "    return pos_encoding\n",
    "\n",
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "# TODO: Tricky look at the code again\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        length = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional encoding.\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x = x + self.pos_encoding[:length, :].unsqueeze(0).to(x.device)\n",
    "        return x\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        # TODO: Implement this when needed\n",
    "        pass\n",
    "\n",
    "class CommonAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, **kwargs):\n",
    "        super(CommonAttention, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, **kwargs)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.add = nn.Identity()  # Add is done through simple addition in PyTorch\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        attn_output, _ = self.mha(query, key, value, **kwargs)\n",
    "        output = self.add(query, attn_output)\n",
    "        output = self.layernorm(output)\n",
    "        return output\n",
    "    \n",
    "class CrossAttention(CommonAttention):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, **kwargs):\n",
    "        super(CrossAttention, self).__init__(embed_dim, num_heads, **kwargs)\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            need_weights=True\n",
    "        )\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = x + attn_output  # Addition operation\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GlobalSelfAttention(CommonAttention):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, **kwargs):\n",
    "        super(GlobalSelfAttention, self).__init__(embed_dim, num_heads, **kwargs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attn_output, _ = self.mha(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x\n",
    "        )\n",
    "        x = x + attn_output  # Addition operation\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "class CausalSelfAttention(CommonAttention):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, **kwargs):\n",
    "        super(CausalSelfAttention, self).__init__(embed_dim, num_heads, **kwargs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Create a causal mask, not sure if this is correct\n",
    "        seq_len = x.size(1)\n",
    "        attn_mask = torch.tril(torch.ones((seq_len, seq_len), device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 0, float('-inf')).masked_fill(attn_mask == 1, float(0.0))\n",
    "\n",
    "        attn_output, _ = self.mha(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + attn_output  # Addition operation\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiyn-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
