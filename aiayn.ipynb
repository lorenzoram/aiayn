{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzoram/aiayn/blob/main/aiayn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "AiXT-h7atiw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper \"Attention Is All You Need\" introduces the Transformer model, a breakthrough in deep learning architecture for tasks involving sequences, such as translation and text generation. The key contributions are:\n",
        "\n",
        "- Attention Mechanisms: The model relies solely on attention mechanisms, dispensing with the need for recurrence (RNNs) or convolutions (CNNs).\n",
        "- Parallelization: The design allows for significant parallelization, making the model more efficient to train.\n",
        "- Performance: The Transformer achieves state-of-the-art results in machine translation tasks, outperforming previous models in both speed and accuracy."
      ],
      "metadata": {
        "id": "CBCZwFioukJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B7cbWv0_tjG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Concepts\n",
        "- Self-Attention: Each word in a sequence is compared to every other word, allowing the model to focus on different parts of the input sequence when generating each word of the output.\n",
        "- Multi-Head Attention: Multiple attention mechanisms run in parallel, enabling the model to capture different aspects of the input sequence simultaneously.\n",
        "\n",
        "Methods\n",
        "- Encoder-Decoder Structure:\n",
        "  - Encoder: Consists of layers that apply self-attention and feed-forward neural networks to transform the input sequence into a set of continuous representations.\n",
        "  - Decoder: Uses these representations to generate the output sequence, applying self-attention to the output and attention to the encoder's output.\n",
        "\n",
        "- Positional Encoding: Adds information about the order of words in the sequence, addressing the lack of sequential information in the model's architecture.\n",
        "\n",
        "Results\n",
        "\n",
        "- BLEU Scores: The Transformer outperforms previous models in BLEU scores (a metric for evaluating the quality of text which has been machine-translated from one language to another), particularly in English-to-German and English-to-French translation tasks.\n",
        "- Efficiency: Due to its parallelizable architecture, the Transformer trains faster and requires less computational resources compared to RNNs and CNNs.\n",
        "\n",
        "By leveraging attention mechanisms, the Transformer model introduces a more efficient and powerful way to handle sequence transduction tasks, setting a new standard in natural language processing."
      ],
      "metadata": {
        "id": "zR2j1ZNbuOUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproduction"
      ],
      "metadata": {
        "id": "FskK3TIAtuh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extension"
      ],
      "metadata": {
        "id": "o2ac4pDNtvkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "4T7m-HlGtySN"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}